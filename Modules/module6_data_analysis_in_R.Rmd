---
title: 'Module 6: Data Analysis in R'
author: "Jasmine Hughes"
date: "9/18/2019"
output: html_document
---


```{r chunksetup, include=FALSE} 
library(fields)
library(haven)
if(!('Modules' %in% unlist(strsplit(getwd(), split = '/')))) setwd('Modules')
gap <- read.csv(file.path('..', 'data', 'gapminder-FiveYearData.csv'), stringsAsFactors = FALSE)
gap2007 <- gap[gap$year == 2007, ]
```


# Dates
- R has built-in ways to handle dates (don't reinvent the wheel!) 

```{r dates}
date1 <- as.Date("03-01-2011", format = "%m-%d-%Y")
date2 <- as.Date("03/02/11", format = "%m/%d/%y")
date3 <- as.Date("07-May-11", format = "%d-%b-%y")

date1; date2
class(date1)
dates <- c(date1, date2, date3)
weekdays(dates)
dates + 30
date3 - date2
as.numeric(dates)
```
(The origin date in R is January 1, 1970)

The `Date` class only works with dates, and not times.

# Datetime

For datetimes, there is the POSIX* classes, POSIXlt and POSIXct, which differ in how the data is stored.

```{r}
# strptime helps convert from character to datetime
date.hour <- strptime("2018-03-17 04:34:12", format = "%Y-%m-%d %H:%M:%S")
date <- c("31/10/2016")
time <- c("13:31:33")
day <- paste(date,"T", time) 
class(day)

#POSIXct stores data as number since origin
dt1 <- as.POSIXct(day,format="%d/%m/%Y T %H:%M:%S",tz="Europe/Paris")
dt1
object.size(dt1)
class(dt1)
attributes(d1)

#POSIXlt stores data as a list
dt2 <- as.POSIXlt(day,format="%d/%m/%Y T %H:%M:%S",tz="Europe/Paris")
dt2
object.size(dt2)
class(dt2)
attributes(dt2)
dt2$year
dt2$hour
dt2$zone
dt2$wday


```

Converting from date/time to hours after dose is pretty common in PK analysis...

```{r datetime_pk}
dose1t <- as.POSIXct("03/02/11 10:15:00", format = "%m/%d/%y %H:%M:%S")
dose2t <- as.POSIXct("03/02/11 18:23:00", format = "%m/%d/%y %H:%M:%S")
difftime(dose2t, dose1t, unit = 'hours')

# Or in dataframe calculations:
data.frame(ID = c(rep(1,3), rep(2,2)),
           datetime = as.POSIXct(c("03/02/11 10:15:00",
                                   "03/02/11 18:23:00",
                                   "03/02/11 23:34:00",
                                   "03/03/11 02:30:00",
                                   "03/03/11 13:24:00"),
                                 format = "%m/%d/%y %H:%M:%S"),
           EVID = c(1, 1, 0, 1, 0),
           DV = c(0, 0, 10.8, 0, 16)) %>%
  group_by(ID) %>%
  mutate(TIME = as.numeric(difftime(datetime, min(datetime), unit = 'hours'))) %>%
  select(ID, TIME, EVID, DV)
  
```

There's lots more packages/functionality for dates/times: see *chron*, *lubridate* and `?DateTimeClasses`

```{r adding_to_dates}
infusion_length <- 3
dose1t + lubridate::hours(infusion_length)
```

# Describing relationships

- Once we've carried out group-wise operations and perhaps reshaped it, we may also like to describe the relationships in the data. Often this involves fitting some style of regression model.  The goal can be pure prediction, description, or inferring a causal relationship between variables.

Of course to infer causality, one has to be quite careful and techniques that try to avoid the usual pitfall that correlation is not causation are way beyond what we can cover here.

We'll just see the basics of how to fit regressions here. 

# Inference/Regression

- Running regressions in R is generally straightforward.

- Most basic, catch-all regression function in R is *glm*

- *glm* fits a generalized linear model with your choice of family/link function (gaussian, logit, poisson, etc.)

- *lm* is just a standard linear regression (equivalent to glm with family = gaussian(link = "identity"))

- The basic glm call looks something like this:

```{r eval=FALSE}
glm(formula = y ~ x1 + x2 + x3 + ..., family = familyname(link = "linkname"),
            data = )
```

- There are a bunch of families and links to use (help(family) for a full list), but some essentials are **binomial(link = "logit")**, **gaussian(link = "identity")**, and **poisson(link = "log")**

If you're using `lm`, the call looks the same but without the `family` argument. 

- Example: suppose we want to regress the life expectency on the GDP per capita and the population, as well as the continent and year.  The lm/glm call would be something like this:

```{r}
reg <- lm(formula = lifeExp ~ log(gdpPercap) + log(pop) + continent + year, 
                data = gap)
summary(reg)
```

# Regression output

- When we store this regression in an object, we get access to several items of interest

```{r}
# View components contained in the regression output
names(reg)
# Examine regression coefficients
reg$coefficients
# Examine regression degrees of freedom
reg$df.residual
# See the standard (diagnostic) plots for a regression
plot(reg)
```

- R has a helpful summary method for regression objects
```{r}
summary(reg)
```

- Can also extract useful things from the summary object

```{r}
# Store summary method results
summ_reg <- summary(reg)
# View summary method results objects
objects(summ_reg)
# View table of coefficients
summ_reg$coefficients
```

- Note that, in our results, R has broken up our variables into their different factor levels (as it will do whenever your regressors have factor levels)

- If your data aren't factorized, you can tell lm/glm to factorize a variable (i.e. create dummy variables on the fly) by writing `factor(character_variable)`

```{r, eval=FALSE}
glm(formula = y ~ x1 + x2 + factor(x3), family = family(link = "link"),
            data = df)
```

# Setting up regression interactions

- There are also some useful shortcuts for regressing on interaction terms:

`x1:x2` interacts all terms in x1 with all terms in x2
```{r}
summary(lm(lifeExp ~ log(gdpPercap) + log(pop) +
                    continent:factor(year), 
                    data = gap))
```

`x1*x2` produces the cross of x1 and x2, or x1+x2+x1:x2
```{r}
summary(lm(lifeExp ~ log(gdpPercap) + log(pop) + continent*factor(year), 
                data = gap))
```


# Smoothing

Linear regression and GLMs are of course useful, but often the relationship is not linear, even on some transformed scale.

Additive models and generalized additive models (GAMs) are the more flexible variants on linear models and GLMs.

There are a variety of tools in R for modeling nonlinear and smooth relationships, mirroring the variety of methods in the literature.

One workhorse is `gam()` in the *mgcv* package.

# GAM in action

Do we think there should be a linear relationship of life expectancy with GDP and year? `s()`: smoothing. (Can specify parameters k, m, etc...)

```{r gamExample, cache=TRUE, fig.width=10, fig.cap = ""}
library(mgcv)

mod <- gam(lifeExp ~ s(gdpPercap, k = 30) + s(year, k = 10), data = gap)

plot(mod)
summary(mod)

mod2 <- gam(lifeExp ~ s(log(gdpPercap), k = 30) + s(year, k = 10), data = gap)
plot(mod2)
```

If we were serious about building a good-fitting model, we could use the same kind of functionality as in lm/glm in terms of factors and interactions.

# How does GAM choose how much to smooth?

GAM uses the data to choose how much smoothing to do. Roughly one can think of what it is doing as carrying out cross-validation and choosing the best amount of smoothing for predicting held-out data.

`k` simply sets an upper bound on the amount of smoothing (you can think if `k` as the number of degrees of freedom - one would be a linear fit).

 - Make sure `k` is less than the number of unique values of the predictor variable
 - The default for `k` is relatively small and in some cases this may overly limit the smoothness of the curve.
    - You can try increasing `k` and see if it increases the fit.
    - If `summary` reports and `edf` that is close to `k` that often suggests that `k` should be increased.
    
# Statistical Tests

Almost any statistical test you can think of is already in R... and the rest are available in other packages.

```{r t_tests}
heights_A <- c(165, 145, 160, 170, 164, 167)
heights_B <- c(172, 181, 175, 165, 168, 178)
height_test <- t.test(heights_A, heights_B)
height_test

attributes(height_test)
height_test$p.value
height_test$conf.int

t.test(heights_A, heights_B, paired = T)

```

```{r normality_test}

s_test <- shapiro.test(heights_B)
s_test
shapiro.test(heights_A)

s_test$p.value

```

```{r anova}

plot(x = factor(gap$continent), y  = gap$lifeExp)

aov <- aov(formula = lifeExp ~ factor(continent), data = gap)

TukeyHSD(aov)

```


# Distributions

Since R was developed by statisticians, it handles distributions and simulation seamlessly.

All commonly-used distributions have functions in R. Each distribution has a family of functions: 

* d - probability density/mass function, e.g. `dnorm()`
* r - generate a random value, e.g., `rnorm()`
* p - cumulative distribution function, e.g., `pnorm()`
* q - quantile function (inverse CDF), e.g., `qnorm()`

Some of the distributions include the following (in the form of their random number generator function): `rnorm()`, `runif()`, `rbinom()`, `rpois()`, `rbeta()`, `rgamma()`, `rt()`, `rchisq()`.

# Distributions in action

```{r, fig.cap = ""}
rnorm(10, mean = 150, sd = 15)
pnorm(1.96)
qnorm(.975)
dbinom(0:10, size = 10, prob = 0.3)
dnorm(5)
dt(5, df = 1)

x <- seq(-5, 5, length = 100)
plot(x, dnorm(x), type = 'l')
```

```{r, fig.cap = ""}
x <- seq(0, 10, length = 100)
plot(x, dchisq(x, df = 1), type = 'l')
```

# Other types of simulation and sampling

We can draw a sample with or without replacement.

```{r}
sample(1:nrow(gap), 20, replace = FALSE)
```

Here's an example of some code that would be part of coding up a bootstrap. As I mentioned previously, this would be a weird dataset to do formal statistical inference on given it includes most of the countries in the world, though one could think about fitting models for the variation over time, treating short-term fluctuations as random.

```{r}
# actual mean
mean(gap$lifeExp, na.rm = TRUE)
# here's a bootstrap sample:
smp <- sample(seq_len(nrow(gap)), replace = TRUE) 
mean(gap$lifeExp[smp], na.rm = TRUE)
```


# The Random Seed

A few key facts about generating random numbers

* Random number generation is based on generating uniformly between 0 and 1 and then transforming to the kind of random number of interest: normal, categorical, etc.
* Random numbers on a computer are *pseudo-random*; they are generated deterministically from a very, very, very long sequence that repeats
* The *seed* determines where you are in that sequence

To replicate any work involving random numbers, make sure to set the seed first.

```{r}
set.seed(1) # this can be whatever number you want.
# I have an untested theory that 42 is over-represented in random seeds.
vals <- sample(1:nrow(gap), 10)
vals
vals <- sample(1:nrow(gap), 10)
vals
set.seed(1)
vals <- sample(1:nrow(gap), 10)
vals
```

 

# Breakout 

### Basics

1) Generate 100 random Poisson values with a population mean of 5. How close is the mean of those 100 values to the value of 5?

2) What is the 95th percentile of a chi-square distribution with 1 degree of freedom?

3) What's the probability of getting a value greater than 5 if you draw from a standard normal distribution? What about a t distribution with 1 degree of freedom?

### Using the ideas

4) Fit two linear regression models from the gapminder data, where the outcome is `lifeExp` and the explanatory variables are `log(pop)`, `log(gdpPercap)`, and `year`. In one model, treat `year` as a numeric variable. In the other, factorize the `year` variable. How do you interpret each model?

5) Consider the code where we used `sample()`.  Initialize a storage vector of 500 zeroes. Set up a bootstrap using a for loop, with 500 bootstrap datasets. Here are the steps within each iteration:

  - resample with replacement a new dataset of the same size as the actual dataset
  - assign the value of the mean of the delay for the bootstrap dataset into the storage vector
  - repeat

Now plot a histogram of the 500 values - this is an estimate of the sampling distribution of the sample mean. 

6) Modify the GAMs of lifeExp on gdpPercap and set `k` to a variety of values and see how the estimated relationships change. What about the estimated uncertainties?

### Advanced 

7) Fit a logistic regression model where the outcome is whether `lifeExp` is greater than or less than 60 years, exploring the use of different predictors.

8) Suppose you wanted to do 10-fold cross-validation for some sort of regression model fit to the *gap* dataset. Write some R code that produces a field in the dataset that indicates which fold each observation is in. Ensure each of the folds has an equal (or as nearly equal as possible if the number of observations is not divisible by 10) number of observations. Hint: consider the *times* argument to the `rep()` function. (If you're not familiar with 10-fold cross-validation, it requires one to divide the dataset into 10 subsets of approximately equal size.)

9) Write some code to demonstrate the central limit theorem. Generate many different replicates of samples of size `n` from a skewed or discrete distribution and show that if `n` is big enough, the distribution of the means (of each sample of size `n`) looks approximately normal in a histogram. I.e., I want you to show that if you have a large number (say 10,000) of means, each mean being the mean of `n` values from a distribution, the distribution of the means looks approximately normal if `n` is sufficiently big. Try doing it without any looping (using techniques from earlier modules)! 



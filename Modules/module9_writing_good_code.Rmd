---
title: 'Module 9: Writing Good Code & Advanced R'
author: "Jasmine Hughes"
date: "9/18/2019"
output: html_document
---

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
if(!('Modules' %in% unlist(strsplit(getwd(), split = '/')))) setwd('Modules')
gap <- read.csv(file.path('..', 'data', 'gapminder-FiveYearData.csv'), stringsAsFactors = FALSE)
```

# Before we start this module...

You know most of what you need to know already!
These rules are just to help you reach your goals with minimal headaches...

Not all of these tips will be immediately useful, but I think the exposure to some
of these concepts will help you know what tools are at your disposal.

# Style Guides
> Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read. - Hadley Wickham

'Advanced R' by Hadley Wickham: http://adv-r.had.co.nz/Style.html

Working groups & companies may have their own!
Google: https://google.github.io/styleguide/Rguide.html

People have strong opinions about this kind of thing:
https://simplystatistics.org/2018/07/27/why-i-indent-my-code-8-spaces/

Overall, Code That Works > Pretty Code... 
... But code is typically read more often than it is written.

# Tips for avoiding bugs

- Plan out your code in advance, including all special cases/possibilities.
  Can you abstract some tasks to make it more general? Can you break down one
  big task into many smaller tasks (and functions)?

- When doing software development, write tests for your code early in
  the process. (There's packages for this! `testit`, `testthat`). 
  Tests are essentially "when I give this function x, I expect y".

- Build up code in pieces, testing along the way. Make big changes in
  small steps, sequentially checking to see if the code has broken on
  test case(s). 

- Analysis should also be built up in pieces. Check your data distributions
  regularly. Peak at the first few lines regularly.

- Code in a modular fashion, making good use of functions, so that you
  don't need to debug the same code multiple times. Smaller functions
  are easier to debug, easier to understand, and can be combined in
  a modular fashion.

- Use core R functionality and algorithms already coded. Figure out
  if a functionality already exists in (or can be adapted from) an R
  package (or potentially in a C/Fortran library/package): code that
  is part of standard mathematical/numerical packages will probably
  be more efficient and bug-free than anything you would write.

- Remove objects you don't need, to avoid accidentally using values
  from an old object via the scoping rules.(`remove()`)

- Be careful that the conditions of *if* statements and the sequences
  of *for* loops are robust when they involve evaluating R code.

- Don't hard code numbers - use variables (e.g., number of iterations,
  parameter values in simulations), even if you don't expect to change
  the value, as this makes the code more readable and reduces bugs when
  you use the same number multiple times. 

- Check that inputs to and outputs from functions (either functions
  you call or functions you write) are valid and use `warning()`
  and `stop()` to give a warning or stop execution when something
  unexpected happens.

- Write code for clarity and accuracy first; then worry about efficiency.
  Write an initial version of the code in the simplest way, without
  trying to be efficient; then make a second version that employs efficiency
  tricks and check that both produce the same output.
  
# Common syntax errors and bugs
- Parenthesis mis-matches
  - `[...]` vs. `[[...]]`
  
- Comparing real numbers exactly using `==` is dangerous. 
  Suppose you generate `x = 0.333333` in some fashion with some code and then check: `x == 1/3`. 
  This will produce FALSE. (Try using `round()` to compare numbers if you are in this situation)

- Vectors vs. single values: 
    + `||` vs. `|` and `&&` vs `&`.
    + You expect a single value but your code gives you a vector
    + You want to compare an entire vector but your code just compares the first value 
      (e.g., in an `if` statement) --   consider using `identical()` or `all.equal()` or `all()` or `any()` etc.

- Silent type conversion when you don't want it, or lack of coercion where you're expecting it

- Using the wrong function or variable name

- Giving unnamed arguments to a function in the wrong order (best practices to name them.... 
  easier to read, but less succinct)
  
- Forgetting to define a variable in the environment of a function and having the function, 
  via R's scoping rules, get that variable as a global variable from one of the enclosing environments. 
  At best the types are not compatible and you get an error; 
  at worst, you use a garbage value and the bug is hard to trace. 
  In some cases your code may work fine when you develop the code 
  (if the variable exists in the enclosing environment), but then may not work 
  when you restart R if the variable no longer exists or is different.

- NULL vs NA. If a value doesn't exist, and you try to compare it (ex, `is.na(NULL)`), you'll get an error.
  Try checking first for NULL before NA (`if (is.null(x) | is.na(x))`).
  
# Catching bugs

There's a function called `debug` that helps you step through your code to find the source of errors.
There's a function called `traceback` that tells you which functions were called, and in what order.

Let's briefly see these tools in action. I'll demo this in a very basic way, but hopefully this
will give you an idea of the power of these tools.

```{r}
buggyFun <- function(myDF) {
   print(names(myDF))
   myDF$id <- seq_len(nrow(myDF))
   sums <- rowSums(myDF)
   return(sums)
}

buggyFun(gap)
# unhelpful error message...

traceback()

debug(buggyFun) #turn on debugger
buggyFun(gap)
undebug(buggyFun) # turn off again

options(error = recover)
buggyFun(gap)

```  

We can also insert ``browser()`` inside a function and R will stop there and allow us to proceed with debugging statements.

# Try/Try Catch

Sometimes you know an error might occur during a calculation - for example a simulation for a
particular patient doesn't converge - and you want to make sure the whole script keeps running,
handling the one error in an elegant way.

Rather than having all of your analyses to grind to a halt because one failed, you want to catch the error, record that it failed, and move on.

The `try()` function is a powerful tool here.

Suppose we tried to do a stratified analysis of life expectancy on GDP within continents, for 2007. I'm going to do this as a for loop for pedagogical reasons, but again, it would be better to do this with dplyr/lapply/by type tools.

For the purpose of illustration, I'm going to monkey a bit with the data such that there is an error in fitting Oceania. This is artificial, but when you stratify data into smaller groups it's not uncommon that the analysis can fail for one of the groups (often because of small sample size or missing data).


```{r}
mod <- list()
fakedat <- gap[gap$year == 2007, ]
fakedat$gdpPercap[fakedat$continent == 'Oceania'] <- NA

for(cont in c('Asia', 'Oceania', 'Europe', 'Americas', 'Africa')) {
  cat("Fitting model for continent ", cont, ".\n")
  tmp <- subset(fakedat, continent == cont)
  mod[[cont]] <- lm(lifeExp ~ log(gdpPercap), data = tmp)
}
```

What happened?

## How we can `try()` harder

```{r}
mod <- list()

for(cont in c('Asia', 'Oceania', 'Europe', 'Americas', 'Africa')) {
    cat("Fitting model for continent ", cont, ".\n")
    tmp <- subset(fakedat, continent == cont)
    curMod <- try(lm(lifeExp ~ log(gdpPercap), data = tmp))
    if(is(curMod, "try-error")) mod[[cont]] <- NA 
               else mod[[cont]] <- curMod            
}

mod[[1]]
mod[[2]]
```
`tryCatch` is similar to `try` but lets you specify how errors and warnings should be handled.

# Testing

Testing should be performed on multiple levels and begun as early as possible
in the development process.  For programs that accept input either from a user
or file, it is important that the code validates the input is what
it expects to receive. Tests that ensure individual code elements (e.g., functions,
classes, and class methods) behave correctly are called *unit tests*.
Writing unit tests early in the process of implementing new functionality
helps you think about what you want a piece of code to do, rather than just how
it does it. This practice improves code quality by focusing your attention
on use cases rather than getting lost in implementation details.

The *testthat* package and *testit* package are helpful for setting up tests. 
Also, *RUnit* is a testing framework for R that helps automate test setup, creation, 
execution, and reporting.  For more information, see Bioconductor's [unit testing guidelines](http://www.bioconductor.org/developers/unitTesting-guidelines/).

# Timing your code

First, a cautionary note...

> premature optimization is the root of all evil
>
> --- Donald Knuth, 1974

There are a few tools in R for timing your code.

```{r cache=TRUE}
system.time(mean(rnorm(1e7)))

```

# Profiling your code

For more advanced assessment of bottlenecks in your code, consider `Rprof()`. Actually, the output
from *Rprof* can be hard to decipher, so you may want to use the *proftools* package functions,
which make use of *Rprof* under the hood. 

Here's a function that does the linear algebra to implement a linear regression, assuming `x`
is the matrix of predictors, including a column for the intercept.


```{r cache=TRUE}
lr_slow <- function(y, x) {
  xtx <- t(x) %*% x
  xty <- t(x) %*% y
  inv <- solve(xtx)   ## explicit matrix inverse is slow and generally a bad idea numerically
  return(inv %*% xty)
}                   

lr_medium <- function(y, x) {
  xtx <- crossprod(x)
  xty <- crossprod(x, y)
  inv <- solve(xtx)   ## explicit matrix inverse is slow and generally a bad idea numerically
  return(inv %*% xty)
}                   

lr_fast <- function(y, x) {
  xtx <- crossprod(x)
  xty <- crossprod(x, y)
  U <- chol(xtx)
  tmp <- backsolve(U, xty, transpose = TRUE)
  return(backsolve(U, tmp))
}                   
```

Now let's try these two functions with profiling turned on.


```{r cache=TRUE}
## generate random observations and random matrix of predictors
y <- rnorm(5000)
x <- matrix(rnorm(5000*1000), nrow = 5000)

library(proftools)

pd1 <- profileExpr(lr_slow(y, x))
hotPaths(pd1)
hotPaths(pd1, value = 'time')

pd2 <- profileExpr(lr_medium(y, x))
hotPaths(pd2)
hotPaths(pd2, value = 'time')

pd3 <- profileExpr(lr_fast(y, x))
hotPaths(pd3)
hotPaths(pd3, value = 'time')

```

You might also check out *profvis* for an alternative to displaying profiling information
generated by *Rprof*.

# Memory use

You should know how much memory (RAM) the computer you are using has and keep in mind how big your objects are and how much memory you code might use. All objects in R are stored in RAM unlike, e.g., SAS or a database.

If in total, the jobs on a machine approach the physical RAM, the machine will start to use the hard disk as 'virtual memory'. This is called paging or swapping, and once this happens you're often toast (i.e., your code may take essentially forever to finish).

Often it's a good idea to roughly estimate how much memory an object will take up even before creating it in R. You can do this with some simple arithmetic. Every real number takes 8 bytes (integers and logicals take less; character strings are complicated), so an object with, say, 1 million rows and 10 columns, all numbers, would take roughly 8 * 1000000 * 10 bytes or 800 Mb.

```{r}
x <- rnorm(1e7)
object.size(x)
1e7*8/1e6  # direct calculation of Mb
print(object.size(x), units = 'auto')
x <- rnorm(1e8)
gc()
rm(x)
gc()
```

# Scripting

* Keep your code in script (i.e., text) files.
* Keep your files modular and focused.
* Write functions to reuse code.
* Learn a powerful, general purpose text editor

If you use a good editor (such as RStudio's built-in editor, emacs with ESS, Aquamacs), it's easier to write and understand your code.

With such editors, you can generally then execute lines or blocks of code easily.

To run all the code in an entire file, do `source('myCodeFile.R')`.

# Good coding practices: functions

Use functions whenever possible. In particular try to write functions 
rather than carry out your work using blocks of code. Why? Functions 
allow us to reuse blocks of code easily for later use and for recreating 
an analysis (reproducible research). It's more transparent than sourcing a file
of code because the inputs and outputs are specified formally, so
you don't have to read through the code to figure out what it does.


Good use of functions includes:

- Write reusable code for core functionality and keep a single copy
of the code (w/ backups of course) so you only need to change it in one place
- Smaller functions are easier to debug, easier to understand, and can
be combined in a modular fashion (like the UNIX utilities). (Goal: 50 or fewer lines)

Functions should: 

 - be modular (having a single task); 
 - have meaningful name; and
 - have a comment describing their purpose, inputs and outputs (see the
   help file for any standard R function for how this is done in that context).
   See `roxygen2` for the package that makes these look pretty. 
   https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html

Two options:
1. Include your functions in your analysis scripts up at the top.
2. Create them in a new .R file, add them to environment with `source('pathtofile/functionfile.R')`

The first is easiest, but once you build up a code base, the second is preferred. 
(And makes it easier to compile functions into a package...)

# Good coding practices: syntax
- Header information: put metainfo on the code into the first few lines
  of the file as comments. Include who, when, what, how the code fits
  within a larger program (if appropriate), possibly the versions of
  R and key packages that you wrote this for

- Indentation: do this systematically (your editor can help here). This
  helps you and others to read and understand the code and can help
  in detecting errors in your code because it can expose lack of symmetry.

- Whitespace: use a lot of it. Some places where it is good to have
  it are (1) around operators (assignment and arithmetic), (2) between
  function arguments and list elements, (3) between matrix/array indices,
  in particular for missing indices. 

- Use blank lines to separate blocks of code and comments to say what
  the block does

- Split long lines at meaningful places. (80 character length is traditional)
  
- Use parentheses for clarity even if not needed for order of operations. 
  For example, `a/y*x` will work but is not easy to read and
  you can easily induce a bug if you forget the order of operations.
  
- Documentation - add lots of comments (but don't belabor the obvious). 
  Remember that in a few months, you may not follow 
  your own code any better than a stranger. Some key things to document: 
    - summarizing a block of code, 
    - explaining a very complicated piece of code 
    - explaining arbitrary constant values.
    - break code into separate files (< 1000 lines per file) with meaningful file names 
      and related functions grouped within a file.
- Choose a consistent naming style for objects and functions: 
  e.g. *numIts* (lowerCamelCase) vs. *NumIts* (UpperCamelCase) vs. *num.its* vs. *num\_its*
    + I'd suggest avoiding periods in names since periods are used for 
      object-oriented programming in R and many other languages
    - Try to have the names be informative without being overly long.
    - Try to avoid using the names of standard R functions for your objects, 
      but R will generally be fairly smart about things.
      
```{r}
c <- 7
c(3,5)
c
rm(c)
c
```

- Use active names for functions (e.g., *calc_loglik*, *calcLogLik*)

# Modern Functional Programming in R (Advanced, if we have time...)

We've looked at `for` loops and programming with functions, we can move on from base R 
to look at more modern and powerful tools for programming provided by 
[`purrr`](http://purrr.tidyverse.org/), a core package in the [`tidyverse`](http://www.tidyverse.org/). `purrr` provides facilities to manipulate datasets using functions in a "tidy" manner. 
Using `purrr` requires familiarity with several other core packages of the `tidyverse`, most notably `dplyr` and `magrittr`. We won't have time to cover details of how to use `purrr` here, but
I highly recommend you look into these packages as you continuing exploring R.

```{r tidyverse}
library(dplyr)
library(purrr)
```

I'll leave you with a simple example of how to use `purrr` to deal with a task
we already looked at with `for` loops. In fact, we'll return to the logistic
regression exercise that we started with.

### A simple example

Recall that we have been fitting regression models for each year. To fit our models across each
of the years separately, we'll first need to put our data in "tidy"
format...

```{r tidy-data}
# let's clean up the data set first
gm_tidy <- gap %>%
  split(.$year)
summary(gm_tidy)
```

Now, we can fit our regression models across each of the years
using `purrr`'s `map`:

```{r glms-error}
gm_lms <- gm_tidy %>%
  purrr::map(~lm(formula = lifeExp ~ log(gdpPercap), data = .))
```

What about protecting ourselves against situations where there wasn't
enough variation in the outcome variable (fewer than two non-missing observations in a year).
(It doesn't happen here but can easily happen in other situations.)

So, can `purrr` handle this? *Yes* - in fact, it's really easy. We can use a
verb called `safely` to separate situations where the GLM succeeds from those
where it doesn't. Let's try it out

```{r glms-safely}
gm_lms <- gm_tidy %>%
  purrr::map(safely(~lm(lifeExp ~ log(gdpPercap), data = .)))
```

_Remark:_ What we accomplish here with `purrr::map` is also easily done using
tools from base R. In fact, using `lapply`, we can evaluate the very same `lm`
formula with our `gap` dataset, albeit without the extra goodies offered by
the pipe (aka `%>%`) syntax and the `safely` convenience, afforded by `magrittr`
and `purrr`, respectively.

Ok. Now, we'll look at the results for one destination, just to get a feel for
the output

```{r glms-purrr-eg}
gm_lms$`2007`
```

Now that we've seen what we can do with `purrr`, it's important to compare this
modern approach to what is offered in base R. (It's very important to understand
and be comfortable with using tools from base R before adopting those that are
offered as part of the `tidyverse`.) To that end, here are some points of
comparison:

* Is `purrr::map` really different from `lapply`, `sapply`, `vapply`?
  * `purrr` is:
    * Consistent in the type of object returned
    * contains many useful shortcuts and convenience functions (e.g., `safely`)
    * Syntax accommodates more complicated iteration schemes.
  * [l/s/v]`apply`:
    * The type of object returned is _not_ always consistent
    * Has fewer dependencies (implented in base R rather than `tidyverse`)
     * Syntax can be unwieldy for more complex use-cases.

For a full comparison of `purrr` versus base R, consider checking out [this
quite thorough
tutorial](https://jennybc.github.io/purrr-tutorial/bk01_base-functions.html) by
Jenny Bryan.

* [Here](http://purrr.tidyverse.org/) you can explore the documentation website
  for the `purrr` package. It includes details about functionality we did not
  have time to discuss and many useful examples that you can use to go further
  with `purrr`.

* [Here](http://www.tidyverse.org/) you can browse the `tidyverse` documentation
  website. It includes an introduction to the core packages, the philosophy of
  this ecosystem of packages, and much more.

# Reproducible research

> An article about computational science in a scientific
publication is **not the scholarship itself**, it is merely
**advertising** of the scholarship. The actual scholarship is the
complete software development environment and the
complete set of instructions which generated the figures.

> --- Jonathan Buckheit and David Donoho, WaveLab and Reproducible Research (1995)

Here are some useful articles talking about reproducibility.

- [Wilson et at., Best practices for scientific computing, ArXiv:1210:0530](http://arxiv.org/abs/1210.0530)
- [Gentzkow and Shapiro tutorial for social scientists](https://web.stanford.edu/~gentzkow/research/CodeAndData.pdf)


# Some ideas for improving reproducibility

- Never change a dataset manually, including in pre-processing and post-processing. 
  Always have a script that operates on the data (or results/output). 
  It's a good idea to have your projects organized into folders, with one folder for your raw data, and
  one folder for your "cleaned" or "processed" data, whatever that might mean.

- Produce figures (e.g., from R) via a script and not by point-and-click.
- When making figures, use `save()` or `save.image()` to save all the inputs needed to recreate a figure, with the code for making the figure in a script file.
- If feasible, include your code for doing analyses and making figures in the relevant document reporting the work by using one of the following tools
    - *R Markdown*
    - *Latex* with *knitr* 
    - *Jupyter* (formerly *IPython Notebook*)
- Always set the random number seed so someone else can duplicate your exact numbers.
- Use version control tools such as Git! 

# Some good books

Many are available online for free!
 
R in general:

* Wickham, Hadley; Advanced R. [https://adv-r.had.co.nz](https://adv-r.had.co.nz) (good coverage of advanced topics in the R language)
* Chambers, John; Software for Data Analysis: Programming with R (available electronically through OskiCat: [https://dx.doi.org/10.1007/978-0-387-75936-4](https://dx.doi.org/10.1007/978-0-387-75936-4)) (More conceptual/theoretical)

Specific aspects of R:

* Wilke, Claus; Fundamentals of Data Visualization. https://serialmentor.com/dataviz/
* Xie, Yihui; Dynamic documents with R and knitr. https://github.com/yihui/knitr-book
* Murrell, Paul; R Graphics, 2nd ed. [https://www.stat.auckland.ac.nz/\~paul/RG2e](https://www.stat.auckland.ac.nz/\~paul/RG2e/)
* Murrell, Paul; Introduction to Data Technologies. [https://www.stat.auckland.ac.nz/\~paul/ItDT/](https://www.stat.auckland.ac.nz/\~paul/ItDT/)
* Wickham, Hadley; R Packages. http://r-pkgs.had.co.nz/ For building your own packages.
